{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard packages\n",
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# custom packages\n",
    "import supplemental_funcs as sf\n",
    "import example_master as EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expensive Computations\n",
    "\n",
    "The purpose of this notebook is to do expensive computations and then save numpy arrays that can be easily loaded by other notebooks without having to do the computations a bunch of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the dictionary of all the numpy arrays that we want to save\n",
    "# each element should be a numpy array\n",
    "big_numpy_dict = {}\n",
    "\n",
    "# save all list\n",
    "# list of (filename, [keys]) to be saved\n",
    "save_all_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file helper\n",
    "def save_np_dict(filename,this_dict,elements):\n",
    "    save_list = {element: this_dict[element] for element in elements}\n",
    "    np.savez(filename,**save_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDE Expensive Computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap KDE Errors\n",
    "\n",
    "Note that I am a little lazy here. Instead of generating a bunch of new samples to compute the MISE, I compute the MISE on bootstraps of the original samples for different samples sizes.\n",
    "\n",
    "These leads to convergence plots which are slightly biased with lower variance. In other words, the convergence is likely slightly faster and less variable than is probably realistic.\n",
    "\n",
    "These plots could be made more accurately with new samples, but since I want to re-use bootstrap samples for constructing the CI, doing so would require more computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load parameters from the example master\n",
    "B_n = 100\n",
    "sample = EM.tri_peak_sample\n",
    "total_n = EM.tri_peak_n\n",
    "N_list = EM.tri_peak_N_list\n",
    "obs_exact_dist = EM.tri_peak_mixture\n",
    "\n",
    "# generate a set of bootstrapped samples\n",
    "Bootstrapped_Samples = np.random.choice(sample,size=[B_n,total_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the samples a few boot strap samples are reasonable\n",
    "plt.hist(Bootstrapped_Samples[80],edgecolor='k',density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the convergence with three different bandwidth parameters $h$ for the GKDE:\n",
    "\n",
    "* **Scott's rule:** $h=\\hat{\\sigma}n^{-1/5}$\n",
    "\n",
    "* **HMISE rule:** $h_{MISE}\\approx \\left(2.09122^{-1/5}\\right)n^{-1/5}$ (multiplicative factor computed in [this notebook](Review%20of%20Density%20Estimation.ipynb))\n",
    "\n",
    "* **1/2 IQR:** $h_{IQRD}\\approx \\left(\\frac{IQR}{2}\\right)n^{-1/5}$ (justification and reasoning provided in  [this notebook](DCI%20and%20Density%20Estimation.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a dictionary of kdes\n",
    "# for each sample index N, there are B kdes corresponding \n",
    "# to each bootstrapped sample\n",
    "\n",
    "# approximate value loaded from example master\n",
    "R_factor = EM.KDE_MISE_factor\n",
    "\n",
    "obs_kde_list = {'Scott':{}, 'HMISE': {}, 'hIQRD': {}}\n",
    "for N in N_list:\n",
    "    obs_kde_list['Scott'][N] = []\n",
    "    obs_kde_list['HMISE'][N] = []\n",
    "    obs_kde_list['hIQRD'][N] = []\n",
    "    \n",
    "    this_hmise = (R_factor/N)**(1/5)\n",
    "    for B_sample in Bootstrapped_Samples:\n",
    "        obs_kde_list['Scott'][N].append(sps.gaussian_kde(B_sample[0:N]))\n",
    "        kde_factor = this_hmise/np.std(B_sample[0:N],ddof=1) # divide to get correct h\n",
    "        obs_kde_list['HMISE'][N].append(sps.gaussian_kde(B_sample[0:N],\n",
    "                                                        bw_method=kde_factor))\n",
    "        \n",
    "        # compute with respect to hIQRD\n",
    "        IQR_dev = sps.iqr(B_sample[0:N])/2\n",
    "        this_hIQRD = IQR_dev*len(B_sample[0:N])**(-1/5)\n",
    "        kde_factor = this_hIQRD/np.std(B_sample[0:N],ddof=1)\n",
    "        obs_kde_list['hIQRD'][N].append(sps.gaussian_kde(B_sample[0:N],\n",
    "                                                        bw_method=kde_factor))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bootstrapped_Samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the MISE for each bootstrapped sample\n",
    "# limit_B = 5 # takes just a subset of samples\n",
    "limit_B = Bootstrapped_Samples.shape[0] # takes all bootstrapped samples\n",
    "\n",
    "obs_err_list = {'Scott': {}, 'HMISE': {}, 'hIQRD': {}}\n",
    "for N in N_list:\n",
    "    obs_err_list['Scott'][N] = []\n",
    "    obs_err_list['HMISE'][N] = []\n",
    "    obs_err_list['hIQRD'][N] = []\n",
    "    \n",
    "    for kde in obs_kde_list['Scott'][N][0:limit_B]:\n",
    "        this_err,tol = sf.L2_err_1D(kde,obs_exact_dist,-10,25,quad_kwargs={'epsabs':1e-6})\n",
    "        obs_err_list['Scott'][N].append(this_err)\n",
    "        \n",
    "    for kde in obs_kde_list['HMISE'][N][0:limit_B]:\n",
    "        this_err, tol = sf.L2_err_1D(kde,obs_exact_dist,-10,25,quad_kwargs={'epsabs':1e-6})\n",
    "        obs_err_list['HMISE'][N].append(this_err)\n",
    "    \n",
    "    for kde in obs_kde_list['hIQRD'][N][0:limit_B]:\n",
    "        this_err, tol = sf.L2_err_1D(kde,obs_exact_dist,-10,25,quad_kwargs={'epsabs':1e-6})\n",
    "        obs_err_list['hIQRD'][N].append(this_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check that results are reasonable\n",
    "err_matrix = np.array([obs_err_list['Scott'][key] for key in obs_err_list['Scott']]).T\n",
    "err_matrix2 = np.array([obs_err_list['HMISE'][key] for key in obs_err_list['HMISE']]).T\n",
    "err_matrix_IQRD = np.array([obs_err_list['hIQRD'][key] for key in obs_err_list['hIQRD']]).T\n",
    "\n",
    "plt.scatter(np.log(N_list*limit_B),np.log(err_matrix.reshape(-1,)))\n",
    "plt.scatter(np.log(N_list*limit_B),np.log(err_matrix2.reshape(-1,)))\n",
    "plt.scatter(np.log(N_list*limit_B),np.log(err_matrix_IQRD.reshape(-1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the KDE errors to dictionary\n",
    "big_numpy_dict['ScottMISE'] = err_matrix\n",
    "big_numpy_dict['OptimalMISE'] = err_matrix2\n",
    "big_numpy_dict['hIQRD_MISE'] = err_matrix_IQRD\n",
    "\n",
    "# add to save all list\n",
    "this_filename = EM.tri_peak_MISE_name\n",
    "save_all_list.append((this_filename,['ScottMISE','OptimalMISE','hIQRD_MISE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # save just the MISE errors\n",
    "# this_filename = EM.tri_peak_MISE_name\n",
    "# save_np_dict(this_filename,big_numpy_dict,['ScottMISE',\n",
    "#                                            'OptimalMISE',\n",
    "#                                            'hIQRD_MISE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Confidence Intervals\n",
    "\n",
    "Here we use the bootrstrapped sample to compute confidence intervals for the GKDE of the tripeak density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pointwise confidence intervals at each of these points\n",
    "this_qx = EM.tri_peak_qx\n",
    "\n",
    "# eval points\n",
    "CI_sample_N = EM.tri_peak_CI_sample_size\n",
    "\n",
    "this_qy_vals = {'Scott':[],'HMISE':[],'hIQRD': []}\n",
    "for key in obs_kde_list:\n",
    "    for kde in obs_kde_list[key][CI_sample_N]:\n",
    "        this_qy_vals[key].append(sf.eval_pdf(this_qx,kde))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save numpy evals to dictionary\n",
    "for key in this_qy_vals.keys():\n",
    "    big_numpy_dict[key+'CI'] = np.array(this_qy_vals[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save just the CI Values: NOTE: We also want to save the CI for \n",
    "# # the update from the bootrstrap as well. This is defined later!\n",
    "# this_filename = EM.tri_peak_CI_name\n",
    "# save_np_dict(this_filename,big_numpy_dict,['ScottCI','HMISECI','hIQRD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expensive BGM and DPMM Computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the $L^2$ for a large sample of BGMM models and a DPMM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the key parameters from the example master\n",
    "sample = EM.tri_peak_sample\n",
    "total_n = EM.tri_peak_n\n",
    "N_list = EM.tri_peak_N_list\n",
    "obs_exact_dist = EM.tri_peak_mixture\n",
    "\n",
    "# number of samples to fit BGMM\n",
    "M = 100\n",
    "\n",
    "# generate a set of bootstrapped samples\n",
    "M_Samples = [obs_exact_dist.rvs(size=[M,N]) for N in N_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get general arguments for EM model\n",
    "BGM_general_arg = EM.tri_peak_BGMM_arg_dict\n",
    "BGM_general_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for distribution A\n",
    "this_K = 5\n",
    "arg_prior_dict_A = {'n_components': this_K,\n",
    "                'weight_concentration_prior_type': 'dirichlet_distribution',\n",
    "                'weight_concentration_prior': 1,\n",
    "                'mean_prior': np.atleast_1d(np.round(np.mean(sample))),\n",
    "                'mean_precision_prior': 1, # kappa\n",
    "                'degrees_of_freedom_prior': 1, # nu\n",
    "                'covariance_prior': np.atleast_2d(np.round(np.cov(sample))) # psi\n",
    "                   }\n",
    "arg_prior_dict_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for distribution C\n",
    "# get a window size that produces a better fit\n",
    "var_min = np.min([EM.distA.var(),EM.distB.var(),EM.distC.var()])\n",
    "window = np.sqrt(var_min)\n",
    "print(window)\n",
    "\n",
    "# scale precision parameter so that variance of the means is \n",
    "# variance of the sample\n",
    "this_kappa0 = window**2/np.cov(sample)\n",
    "print(this_kappa0)\n",
    "\n",
    "# define new prior with adjusted window\n",
    "arg_prior_dict_C = arg_prior_dict_A.copy()\n",
    "arg_prior_dict_C['covariance_prior'] = np.atleast_2d(window)\n",
    "arg_prior_dict_C['mean_precision_prior'] = this_kappa0\n",
    "print(arg_prior_dict_C)\n",
    "print()\n",
    "\n",
    "# define prior for DPMM\n",
    "arg_prior_dict_DP_C = arg_prior_dict_C.copy()\n",
    "arg_prior_dict_DP_C['n_components'] = 30\n",
    "arg_prior_dict_DP_C['weight_concentration_prior_type'] = 'dirichlet_process'\n",
    "print(arg_prior_dict_DP_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a dictionary of BGM models\n",
    "# for each sample index N, there is one BGMM model\n",
    "\n",
    "obs_BGM_list = {'BGMM_A':{}, 'BGMM_C': {}, 'DPMM_C': {}}\n",
    "for N in N_list:\n",
    "    obs_BGM_list['BGMM_A'][N] = {'model': BayesianGaussianMixture(**arg_prior_dict_A,\n",
    "                                            **BGM_general_arg)}\n",
    "    obs_BGM_list['BGMM_C'][N] = {'model': BayesianGaussianMixture(**arg_prior_dict_C,\n",
    "                                            **BGM_general_arg)}\n",
    "    \n",
    "    obs_BGM_list['DPMM_C'][N] = {'model': BayesianGaussianMixture(**arg_prior_dict_C,\n",
    "                                            **BGM_general_arg)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a subset of samples\n",
    "# limit_BGM = 2 # takes just a subset of samples\n",
    "limit_BGM = 100 # make it similar to the bootstrap estimate\n",
    "\n",
    "# fit all of the BGM models\n",
    "for key in obs_BGM_list:\n",
    "    for nj,N in enumerate(N_list):\n",
    "        # this model and samples\n",
    "        this_model = obs_BGM_list[key][N]['model']\n",
    "        these_samples = M_Samples[nj]\n",
    "        \n",
    "        # dictionary to save parameters\n",
    "        obs_BGM_list[key][N]['param_sample'] = {'weight': [],\n",
    "                                                'mean': [],\n",
    "                                                'cov': []}\n",
    "        \n",
    "        # for each of the M samples\n",
    "        for sample in these_samples[0:limit_BGM]:\n",
    "            # fit the model to this specific dateset\n",
    "            this_model.fit(sample.reshape(-1,1))\n",
    "#             print('{}, {}: '.format(key,N),obs_BGM_list[key][N]['model'].converged_)\n",
    "            \n",
    "            # save the weights, means and covariances\n",
    "            obs_BGM_list[key][N]['param_sample']['weight'].append(np.squeeze(this_model.weights_))\n",
    "            obs_BGM_list[key][N]['param_sample']['mean'].append(np.squeeze(this_model.means_))\n",
    "            obs_BGM_list[key][N]['param_sample']['cov'].append(np.squeeze(this_model.covariances_))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # for each of the BGMs generate limit_B random pdf samples\n",
    "# for key in obs_BGM_list:\n",
    "#     for N in N_list:\n",
    "#         this_BGM = obs_BGM_list[key][N]['model']\n",
    "#         this_sample_param = sf.Forward_BGM_Model(this_BGM).rvs(limit_BGM)\n",
    "#         obs_BGM_list[key][N]['param_sample'] = this_sample_param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the mixture pdfs and compute L2 error for each\n",
    "BGM_err_list = {}\n",
    "for key in obs_BGM_list:\n",
    "    BGM_err_list[key] = {}\n",
    "    this_err_array = np.empty([limit_BGM,len(N_list)])\n",
    "    for nj,N in enumerate(N_list):\n",
    "        # get the parameter weights and means\n",
    "        this_weight = obs_BGM_list[key][N]['param_sample']['weight']\n",
    "        this_mean = np.squeeze(obs_BGM_list[key][N]['param_sample']['mean'])\n",
    "        this_var = obs_BGM_list[key][N]['param_sample']['cov']\n",
    "        \n",
    "        for ib, (mu,sig2,w) in enumerate(zip(this_mean,this_var,this_weight)):\n",
    "            # get the mixture pdfs\n",
    "            pdfs = [sps.norm(m,np.sqrt(s2)) for m,s2 in zip(mu,sig2)]\n",
    "            this_mixture_dist = sf.mixture_dist(pdfs,w)\n",
    "            \n",
    "            # get the errors and save them\n",
    "            this_err_array[ib,nj], tol = sf.L2_err_1D(obs_exact_dist,\n",
    "                                              this_mixture_dist,-10,25)\n",
    "    # save error array to dictionary\n",
    "    BGM_err_list[key]['L2_err'] = this_err_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check errors make sense\n",
    "err_matrix3 = BGM_err_list['BGMM_A']['L2_err']\n",
    "err_matrix4 = BGM_err_list['BGMM_C']['L2_err']\n",
    "err_matrix5 = BGM_err_list['DPMM_C']['L2_err']\n",
    "\n",
    "plt.scatter(np.log(N_list*limit_BGM),np.log(err_matrix3.reshape(-1,1)),label='BGMM_A')\n",
    "plt.scatter(np.log(N_list*limit_BGM),np.log(err_matrix4.reshape(-1,1)),label='BGMM_C')\n",
    "plt.scatter(np.log(N_list*limit_BGM),np.log(err_matrix5.reshape(-1,1)),label='DPMM_C')\n",
    "# plt.scatter(np.log(N_list*5),np.log(err_matrix.reshape(-1,)),label='ScottKDE')\n",
    "# plt.scatter(np.log(N_list*5),np.log(err_matrix2.reshape(-1,)),label='HMISEkde')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the BGM errors to file\n",
    "big_numpy_dict['BGMM_A_L2_err'] = BGM_err_list['BGMM_A']['L2_err']\n",
    "big_numpy_dict['BGMM_C_L2_err'] = BGM_err_list['BGMM_C']['L2_err']\n",
    "big_numpy_dict['DPMM_C_L2_err'] = BGM_err_list['DPMM_C']['L2_err']\n",
    "\n",
    "# add to save all list\n",
    "this_filename = EM.tri_peak_BGM_name\n",
    "save_all_list.append((this_filename,['BGMM_A_L2_err','BGMM_C_L2_err','DPMM_C_L2_err']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save just the CI Values\n",
    "# this_filename = EM.tri_peak_BGM_name\n",
    "# save_np_dict(this_filename,big_numpy_dict,['BGMM_A_L2_err',\n",
    "#                                            'BGMM_C_L2_err',\n",
    "#                                            'DPMM_C_L2_err'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Consistent Update MSE\n",
    "\n",
    "In this section, we do expensive computations for the update. We use some of the density estimations of the observed distribution from the previous section here to do the analysis of the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Qmap\n",
    "Q_map = EM.Q_nonlinear_1D_to_1D\n",
    "\n",
    "# setup initial, predicted density, and observed density\n",
    "init_dist = sps.beta(a=1,b=1.5,scale=10)\n",
    "predict_sample = Q_map(init_dist.rvs(5000))\n",
    "predict_kde = sps.gaussian_kde(predict_sample)\n",
    "obs_dist = EM.tri_peak_mixture\n",
    "\n",
    "# save the appropriate domains\n",
    "lamx = EM.tri_peak_lamx\n",
    "qx = EM.tri_peak_qx\n",
    "\n",
    "# define exact update\n",
    "exact_update = sf.dci_update(init_dist,predict_kde,obs_dist,Q_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check distributions are correct\n",
    "fig_exact_update, (axL,axD) = plt.subplots(1,2)\n",
    "fig_exact_update.set_figwidth(9)\n",
    "\n",
    "# parameter space\n",
    "axL.plot(lamx,init_dist.pdf(lamx),ls='--',color='gray',\n",
    "         alpha=0.7,label='Initial')\n",
    "axL.plot(lamx,exact_update.pdf(lamx),label='Update')\n",
    "\n",
    "\n",
    "# data space\n",
    "axD.plot(qx,predict_kde.pdf(qx),ls='--',color='gray',label='Predicted')\n",
    "axD.plot(qx,obs_dist.pdf(qx),label='Observed')\n",
    "\n",
    "# typical labels\n",
    "axL.legend()\n",
    "axL.set_title('Parameter Space $\\Lambda$')\n",
    "axL.set_xlabel('$\\lambda$')\n",
    "axD.legend()\n",
    "axD.set_title('Data Space $\\mathcal{D}$')\n",
    "axD.set_xlabel('$q$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the $L1$ and $L2$ errors for the update using the bootstrapped samples of KDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the MISE for each bootstrapped sample\n",
    "# limit_B = 5 # takes just a subset of samples\n",
    "limit_B = Bootstrapped_Samples.shape[0] # takes all bootstrapped samples\n",
    "\n",
    "update_err_list = {'L1': {}, 'L2': {}}\n",
    "for N in N_list:\n",
    "    update_err_list['L1'][N] = []\n",
    "    update_err_list['L2'][N] = []\n",
    "    \n",
    "    for kde in obs_kde_list['hIQRD'][N][0:limit_B]:\n",
    "        # define the updated distribution for this KDE\n",
    "        this_update = sf.dci_update(init_dist,predict_kde,kde,Q_map)\n",
    "        \n",
    "        # compute the L2 error for the update\n",
    "        this_err_L2, tol = sf.L2_err_1D(this_update,exact_update,0.01,10,quad_kwargs={'epsabs':1e-6})\n",
    "        update_err_list['L2'][N].append(this_err_L2)\n",
    "        \n",
    "        # compute the L1 error for the update\n",
    "        this_err_L1, tol = sf.L1_err_1D(this_update,exact_update,0.01,10,quad_kwargs={'epsabs':1e-6})\n",
    "        update_err_list['L1'][N].append(this_err_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that results are reasonable\n",
    "lam_err_matrix = np.array([update_err_list['L1'][key] for key in update_err_list['L1']]).T\n",
    "lam_err_matrix2 = np.array([update_err_list['L2'][key] for key in update_err_list['L2']]).T\n",
    "\n",
    "# plt.scatter(np.log(N_list*limit_B),np.log(lam_err_matrix.reshape(-1,)))\n",
    "plt.scatter(np.log(N_list*limit_B),np.log(lam_err_matrix2.reshape(-1,)))\n",
    "plt.scatter(np.log(N_list*limit_B),np.log(err_matrix_IQRD.reshape(-1,)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the KDE errors to dictionary\n",
    "big_numpy_dict['updateMISE_L1'] = lam_err_matrix\n",
    "big_numpy_dict['updateMISE_L2'] = lam_err_matrix2\n",
    "\n",
    "# add to save all list\n",
    "this_filename = EM.tri_peak_update_MISE_name\n",
    "save_all_list.append((this_filename,['updateMISE_L1','updateMISE_L2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save just the MISE errors\n",
    "# this_filename = EM.tri_peak_update_MISE_name\n",
    "# save_np_dict(this_filename,big_numpy_dict,['updateMISE_L1',\n",
    "#                                            'updateMISE_L2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute MSE and L1 for DPMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_dict = EM.tri_peak_BGMM_arg_dict\n",
    "this_K = 15\n",
    "\n",
    "arg_prior_dict_DPMM_UP = {'n_components': this_K,\n",
    "                'weight_concentration_prior_type': 'dirichlet_process',\n",
    "                'weight_concentration_prior': 1,\n",
    "                'mean_prior': np.atleast_1d(1),\n",
    "                'mean_precision_prior': 1, # kappa\n",
    "                'degrees_of_freedom_prior': 1, # nu\n",
    "                'covariance_prior': np.atleast_2d(np.round(IQR_dev**2)) # psi\n",
    "                   }\n",
    "\n",
    "print(arg_prior_dict_DPMM_UP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the MISE for each bootstrapped sample\n",
    "print(limit_B) # takes just a subset of samples\n",
    "# limit_B = Bootstrapped_Samples.shape[0] # takes all bootstrapped samples\n",
    "\n",
    "DPMM_update_err_list = {'L1q': {}, 'L2q': {}, 'L2lam': {}}\n",
    "for N in N_list:\n",
    "    DPMM_update_err_list['L1q'][N] = []\n",
    "    DPMM_update_err_list['L2q'][N] = []\n",
    "    DPMM_update_err_list['L2lam'][N] = []\n",
    "    \n",
    "    for B_sample in Bootstrapped_Samples[0:limit_B]:\n",
    "        # update the covariance prior\n",
    "        this_sample = B_sample[0:N]\n",
    "        IQR_dev = sps.iqr(this_sample)/2\n",
    "        arg_prior_dict_DPMM_UP['covariance_prior'] = np.atleast_2d(np.round(IQR_dev**2))\n",
    "        \n",
    "        # define and fit the model\n",
    "        this_DPMM = BayesianGaussianMixture(**arg_prior_dict_DPMM_UP,**arg_dict)\n",
    "        this_DPMM.fit(this_sample.reshape(-1,1))\n",
    "        \n",
    "        # compute error in qspace\n",
    "        this_DPMM_Forward = sf.Forward_BGM_Model(this_DPMM)\n",
    "        this_err_L1, tol = sf.L1_err_1D(this_DPMM_Forward,obs_dist,-10,25,\n",
    "                                           quad_kwargs={'epsabs':1e-6})\n",
    "        this_err_L2, tol = sf.L2_err_1D(this_DPMM_Forward,obs_dist,-10,25,\n",
    "                                           quad_kwargs={'epsabs':1e-6})\n",
    "        # save data space errors\n",
    "        DPMM_update_err_list['L1q'][N].append(this_err_L1)\n",
    "        DPMM_update_err_list['L2q'][N].append(this_err_L2)\n",
    "        \n",
    "        \n",
    "        # compute the L2 error for the update\n",
    "        this_update = sf.dci_update(init_dist,predict_kde,this_DPMM,Q_map)\n",
    "        this_up_err_L2, tol = sf.L2_err_1D(this_update,exact_update,0.01,10,quad_kwargs={'epsabs':1e-6})\n",
    "        DPMM_update_err_list['L2lam'][N].append(this_up_err_L2)\n",
    "        \n",
    "#         # compute the L1 error for the update\n",
    "#         this_err_L1, tol = sf.L1_err_1D(this_update,exact_update,0.01,10,quad_kwargs={'epsabs':1e-6})\n",
    "#         update_err_list['L1q'][N].append(this_err_L1)\n",
    "        \n",
    "#         # define the updated distribution for this KDE\n",
    "#         this_update = sf.dci_update(init_dist,predict_kde,kde,Q_map)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPMM_update_err_list['L1q']\n",
    "\n",
    "DPMM_err_matrix1 = np.array([DPMM_update_err_list['L1q'][key] for key in DPMM_update_err_list['L1q']]).T\n",
    "DPMM_err_matrix2 = np.array([DPMM_update_err_list['L2q'][key] for key in DPMM_update_err_list['L2q']]).T\n",
    "DPMM_err_matrix3 = np.array([DPMM_update_err_list['L2lam'][key] for key in DPMM_update_err_list['L2lam']]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.log(N_list*limit_B),np.log(DPMM_err_matrix1.reshape(-1,)))\n",
    "plt.scatter(np.log(N_list*limit_B),np.log(DPMM_err_matrix2.reshape(-1,)))\n",
    "plt.scatter(np.log(N_list*limit_B),np.log(DPMM_err_matrix3.reshape(-1,)))\n",
    "\n",
    "# plt.scatter(np.log(N_list*limit_B),np.log(err_matrix.reshape(-1,)))\n",
    "plt.scatter(np.log(N_list*limit_B),np.log(err_matrix_IQRD.reshape(-1,)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the KDE errors to dictionary\n",
    "big_numpy_dict['DPMM_err_L1'] = DPMM_err_matrix1\n",
    "big_numpy_dict['DPMM_err_L2'] = DPMM_err_matrix2\n",
    "big_numpy_dict['DPMM_up_err_L2'] = DPMM_err_matrix3\n",
    "\n",
    "# add to save all list\n",
    "this_filename = EM.tri_peak_DPMM_name\n",
    "save_all_list.append((this_filename,['DPMM_err_L1','DPMM_err_L2','DPMM_up_err_L2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save just the MISE errors\n",
    "# this_filename = EM.tri_peak_DPMM_name\n",
    "# save_np_dict(this_filename,big_numpy_dict,['DPMM_err_L1',\n",
    "#                                            'DPMM_err_L2',\n",
    "#                                            'DPMM_up_err_L2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Confidence Intervals for the Update\n",
    "\n",
    "Use the Bootrstrapped KDES to compute the bootstrapped CI for the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pointwise confidence intervals at each of these points\n",
    "this_lamx = EM.tri_peak_lamx\n",
    "this_qx = EM.tri_peak_qx\n",
    "\n",
    "# eval points\n",
    "CI_sample_N = EM.tri_peak_CI_sample_size\n",
    "\n",
    "this_lamy_vals = {'hIQRD': []}\n",
    "for key in this_lamy_vals:\n",
    "    for kde in obs_kde_list[key][CI_sample_N]:\n",
    "        this_update = sf.dci_update(init_dist,predict_kde,kde,Q_map)\n",
    "        this_lamy_vals[key].append(sf.eval_pdf(this_lamx,this_update))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save numpy evals to dictionary\n",
    "big_numpy_dict['updateCI'] = np.array(this_lamy_vals['hIQRD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to make sure all the keys are in the dict\n",
    "for key in ['ScottCI','HMISECI','hIQRDCI','updateCI']:\n",
    "    print(big_numpy_dict[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to save all list\n",
    "this_filename = EM.tri_peak_CI_name\n",
    "save_all_list.append((this_filename,['ScottCI','HMISECI','hIQRDCI','updateCI']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save ALL the CI Values\n",
    "# this_filename = EM.tri_peak_CI_name\n",
    "# save_np_dict(this_filename,big_numpy_dict,['ScottCI','HMISECI',\n",
    "#                                            'hIQRDCI','updateCI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Save of Numpy Arrays!\n",
    "\n",
    "Better to use individual saves, but may be used to save all the numpy arrays at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the save all list before saving\n",
    "save_all_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename,these_keys in save_all_list:\n",
    "#     save_np_dict(filename,big_numpy_dict,these_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
