{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessary Packages\n",
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "import pandas as pd\n",
    "\n",
    "# Specific Plotting Packages\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib import patches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "# other specific packages\n",
    "from scipy.integrate import quad\n",
    "from scipy.special import gamma as gamma_func\n",
    "import warnings\n",
    "\n",
    "# Import Custom Packages\n",
    "from distr_tools import mixture_dist\n",
    "import supplemental_funcs as sf\n",
    "import example_master as EM # sets values for example\n",
    "\n",
    "# to show in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_save_dictionary { 'filename' : figname }\n",
    "fig_all_master = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Density Estimation\n",
    "\n",
    "In this notebook, we go through and review the following density estimation techniques for the observed distribution from a finite sample, producing figures as we go.\n",
    "\n",
    "1. Kernel Density Estimates\n",
    "2. Bayesian Mixture Model\n",
    "3. Dirichlet Process Mixture Model\n",
    "\n",
    "For these examples, we will use the following distribution and sample for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get observed target distribution and sample from parameter file\n",
    "obs_dist = EM.tri_peak_mixture\n",
    "obs_sample_full = EM.tri_peak_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qx = EM.tri_peak_qx\n",
    "plt.plot(qx,obs_dist.pdf(qx),label='Target Density')\n",
    "plt.hist(obs_sample_full, density=True,edgecolor='k',\n",
    "         color='xkcd:sky blue', alpha=0.25,\n",
    "         label='Full Sample n={}'.format(len(obs_sample_full)))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This density above is a mixture model (see [`example_master.py`](example_master.py)) with the following parameters:\n",
    "\n",
    "\\begin{align}\n",
    "\\pi^{obs}(q)&= \\sum_{k=1}^3 w_k f_k(q) \\\\\n",
    "\\\\\n",
    "f_1(q)&\\sim 3(\\text{Beta}(2,5))-2 \\\\\n",
    "f_2(q)&\\sim \\text{Truncnorm}(\\mu=2.5,\\sigma=1.5,[-2,\\infty]) \\\\\n",
    "f_3(q)&\\sim \\text{Truncnorm}(\\mu=10,\\sigma=2.5,[-2,\\infty]) \\\\\n",
    "\\\\\n",
    "w &= \\left(\\frac{2}{10},\\frac{5}{10},\\frac{3}{10}\\right) \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Density Estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we just illustrate what a KDE looks like using the sum of bumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_n = 5\n",
    "# rand_ind = np.random.randint(0,100,this_n)\n",
    "# print(rand_ind)\n",
    "ind_ex = np.array([34,57,10,96,13]) # choose some points for example\n",
    "this_obs = obs_sample_full[ind_ex]\n",
    "small_h_factor = this_n**(-1/5)/np.std(this_obs,ddof=1)\n",
    "this_kde_list = [sps.gaussian_kde(this_obs),\n",
    "                 sps.gaussian_kde(this_obs,bw_method=small_h_factor)]\n",
    "\n",
    "# list of bandwidth values used in the kde\n",
    "h_list = [this_kde_list[1].scotts_factor()*np.std(this_obs,ddof=1),\n",
    "          small_h_factor*np.std(this_obs,ddof=1)]\n",
    "\n",
    "\n",
    "def kde_bump(x,mu,h):\n",
    "    return 1/(this_n*h)*sps.norm.pdf((x-mu)/h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_kde_describe, axes = plt.subplots(1,2,sharey=False)\n",
    "fig_kde_describe.set_figwidth(10)\n",
    "shift = 3\n",
    "this_q = qx-shift\n",
    "\n",
    "for ax,this_kde,this_h in zip(axes,this_kde_list,h_list):\n",
    "    ax.plot(this_q,this_kde(this_q),label='GKDE')\n",
    "\n",
    "    # plot the bump functions\n",
    "    \n",
    "    for i,q in enumerate(this_obs):\n",
    "        this_bump_qx = kde_bump(this_q,this_obs[i],this_h) \n",
    "        ax.plot(this_q,this_bump_qx,color='xkcd:salmon',zorder=-1)\n",
    "\n",
    "        this_bump_max = kde_bump(this_obs[i],this_obs[i],this_h)\n",
    "        ax.vlines(this_obs[i],ymin=0,ymax=this_bump_max,color='k',ls='--')\n",
    "    ax.legend()\n",
    "    ax.set_title('Gaussian KDE, $h={:0.2}$'.format(this_h))\n",
    "\n",
    "# save fig\n",
    "this_fig_title = 'fig_kde_describe.png'\n",
    "fig_all_master[this_fig_title] = fig_kde_describe\n",
    "\n",
    "# # save just this figure\n",
    "# fig_kde_describe.savefig('../'+this_fig_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fit to Example Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we want to compare the convergence rates of HMISE versus Scott's rule.\n",
    "\n",
    "Since we know the sampling distribution, we can approximate $h_{AMISE}$ fairly well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the $h_{MISE}$\n",
    "\n",
    "From Zambom and Dias (2012), in 1D we know that $h_{MISE}$ for a KDE is\n",
    "\n",
    "\\begin{align}\n",
    "h_{MISE}=\\left( \\dfrac{R(K)}{\\mu_2^2(K) R(\\pi'')}\\right)^{1/5}n^{-1/5}\n",
    "\\end{align}\n",
    "\n",
    "For a Gaussian kernel, $N(0,1)$, we have \n",
    "\n",
    "\\begin{align}\n",
    "R(K)&=(2\\sqrt{\\pi})^{-1} \\\\\n",
    "\\mu_2^2(K) &= \\int_{-\\infty}^{\\infty}x^2 K(x) dx = Var_K(X)-E_K(X)^2 = 1\n",
    "\\end{align}\n",
    "since $K(x)$ is a \n",
    "\n",
    "For this example, we want to estimate the integral\n",
    "\n",
    "\\begin{align}\n",
    "R(\\pi'')&=\\int_\\mathcal{D} (\\pi''(q))^2 d\\mu_q\n",
    "\\end{align}\n",
    "\n",
    "Since we know the target function in this case, we compute the derivatives by hand and use `scipy.integrate.quad` to estimate the integral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $f(x)\\sim\\text{Beta}(2,5)$, then\n",
    "\n",
    "\\begin{align}\n",
    "f'(x)&= \\frac{d}{dx}\\left(\\frac{1}{B(2,5)}x^{2-1}(1-x)^{5-1}\\right) \\\\\n",
    "&=\\frac{1}{B(2,5)}\\left(-4x(1-x)^3+(1-x)^4\\right) \\\\\n",
    "\\Rightarrow\\quad f''(x)&=\\frac{1}{B(2,5)}\\left(\\left(12x(1-x)^2-4(1-x)^3\\right)-4(1-x)^3\\right) \\\\\n",
    "&=\\frac{1}{B(2,5)}\\left(12x(1-x)^2-8(1-x)^3\\right) \\\\\n",
    "&=\\frac{1}{B(2,5)}\\cdot 4\\left(3x-2(1-x)\\right)(1-x)^2 \\\\\n",
    "&=\\frac{4}{B(2,5)} \\left(5x-2\\right)(1-x)^2 \n",
    "\\end{align}\n",
    "\n",
    "where $B(2,5)$ is a constant of integration related to the Gamma function.\n",
    "\n",
    "Specifically,\n",
    "\\begin{align}\n",
    "B(2,5) = \\frac{\\Gamma(2)\\Gamma(5)}{\\Gamma(2+5)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $f(x)\\sim \\text{TruncStandardNormal}$, we know that \n",
    "\n",
    "\\begin{align}\n",
    "f(x)&=\\frac{1}{C}\\phi(x)\n",
    "\\end{align}\n",
    "\n",
    "where $\\phi(x)$ is a standard normal distribution and $C$ is a constant of integration (the proportion of area not truncated).\n",
    "\n",
    "So,\n",
    "\n",
    "\\begin{align}\n",
    "f(x) &=\\frac{1}{C\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right) \\\\\n",
    "\\Rightarrow\\quad f'(x) &= \\frac{1}{C\\sqrt{2\\pi}}\\cdot -x\\cdot \\exp\\left(-\\frac{x^2}{2}\\right) \\\\\n",
    "\\Rightarrow\\quad f''(x) &= -\\frac{1}{C\\sqrt{2\\pi}}\\cdot \\left( x\\cdot (-x)\\cdot \\exp\\left(-\\frac{x^2}{2}\\right) + \\exp\\left(-\\frac{x^2}{2}\\right)\\right) \\\\\n",
    "&=-\\frac{1}{C\\sqrt{2\\pi}}\\cdot \\left( \\exp\\left(-\\frac{x^2}{2}\\right) - x^2\\cdot \\exp\\left(-\\frac{x^2}{2}\\right) \\right) \\\\\n",
    "&=\\frac{1}{C\\sqrt{2\\pi}}\\cdot \\left( x^2-1\\right) \\exp\\left(-\\frac{x^2}{2}\\right)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used standard distributions, but now I need to transform the derivatives by a location scale function $y=g(x)=\\frac{(x-\\mu)}{\\sigma}$.\n",
    "\n",
    "For these transformations,\n",
    "\n",
    "\\begin{align}\n",
    "f_Y(y) = \\frac{f_X(g^{-1}(y))}{\\sigma}\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align}\n",
    "g^{-1}(y)&=\\sigma y + \\mu \\\\\n",
    "g'^{-1}(y) &= \\sigma\n",
    "\\end{align}\n",
    "\n",
    "Thus, using the chain rule, we can find that\n",
    "\n",
    "\\begin{align}\n",
    "f_Y'(y) &= \\frac{1}{\\sigma}\\left(f_X'(g^{-1}(y))g'^{-1}(y)\\right) \\\\\n",
    "f_Y''(y) &= \\frac{1}{\\sigma}\\left(f_X''(g^{-1}(y))g'^{-1}(y)+f_X'(g^{-1}(y))g''^{-1}(y)\\right) \\\\\n",
    "&=\\left(\\frac{1}{\\sigma}\\right)f_X''(g^{-1}(y))g'^{-1}(y) \\\\\n",
    "&=f_X''(g^{-1}(y))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the second derivatives\n",
    "def beta_prime_prime(t,alpha=2,beta=5):\n",
    "    x = np.array(t, dtype=float)\n",
    "    \n",
    "    term1 = (alpha-2)*(alpha-1)*x**(alpha-3)*(1-x)**(beta-1)\n",
    "    term2 = -2*(alpha-1)*(beta-1)*x**(alpha-2)*(1-x)**(beta-2)\n",
    "    term3 = (beta-2)*(beta-1)*x**(alpha-1)*(1-x)**(beta-3)\n",
    "    \n",
    "    term_sum = term1+term2+term3\n",
    "    \n",
    "#     if np.any(x==0):\n",
    "#         if x.shape == ():\n",
    "#             term_sum = 0\n",
    "#         else:\n",
    "#             term_sum[x==0] = 0\n",
    "    \n",
    "    if x.shape == ():\n",
    "        if x > 1:\n",
    "            term_sum = 0\n",
    "        elif x < 0:\n",
    "            term_sum = 0\n",
    "    else:\n",
    "        term_sum[x>1]=np.zeros_like(term_sum[x>1])\n",
    "        term_sum[x<=0]=np.zeros_like(term_sum[x<=0])\n",
    "    \n",
    "    weight = gamma_func(alpha)*gamma_func(beta)/gamma_func(alpha+beta)\n",
    "\n",
    "    return term_sum*weight\n",
    "\n",
    "def truncnorm_prime_prime(t,trunc_const):\n",
    "    x = np.array(t, dtype=float)\n",
    "    term = (x**2-1)*np.exp(-x**2/2)\n",
    "    weight = 1/np.sqrt(2*np.pi)*1/trunc_const\n",
    "    return term*weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the specific function for the integral\n",
    "truncB = 1-sps.norm.cdf(EM.distB.a)\n",
    "truncC = 1-sps.norm.cdf(EM.distC.a)\n",
    "\n",
    "def true_pi_prime_prime(q):  \n",
    "    x = np.array(q, dtype=float)\n",
    "    out = np.zeros_like(x)\n",
    "    \n",
    "    yA = (x- EM.distA.kwds['loc'])/EM.distA.kwds['scale']\n",
    "    out += EM.dist_weights[0]*beta_prime_prime(yA)\n",
    "    \n",
    "    yB = (x- EM.distB.kwds['loc'])/EM.distB.kwds['scale']\n",
    "    out += EM.dist_weights[1]*truncnorm_prime_prime(yB,truncB)\n",
    "    \n",
    "    yC = (x- EM.distC.kwds['loc'])/EM.distC.kwds['scale']\n",
    "    out += EM.dist_weights[2]*truncnorm_prime_prime(yC,truncC)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sample sizes for evaluation\n",
    "N_list = EM.tri_peak_N_list\n",
    "print(N_list, '\\n')\n",
    "\n",
    "# compute the squared integral\n",
    "f = lambda q: true_pi_prime_prime(q)**2\n",
    "Rprimeprime,err = quad(f,-2,40)\n",
    "\n",
    "# compute hmise\n",
    "RK = 1/(2*np.sqrt(np.pi))\n",
    "mu2K = 1\n",
    "print(RK/(mu2K*Rprimeprime))\n",
    "\n",
    "h_mise_list = []\n",
    "for N in N_list:\n",
    "    h_mise = (1/N*(RK/(mu2K*Rprimeprime)))**(1/5)\n",
    "    h_mise_list.append(h_mise)\n",
    "    print(h_mise,'\\t', np.std(obs_sample_full[0:N],ddof=1)*N**(-1/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_kde_dict = {'Scott':[],'HMISE':[]}\n",
    "for i,N in enumerate(N_list):\n",
    "    this_sample = obs_sample_full[0:N]\n",
    "    \n",
    "    # kde using scott's rule\n",
    "    scott_kde = sps.gaussian_kde(this_sample)\n",
    "    this_kde_dict['Scott'].append(scott_kde)\n",
    "    \n",
    "    # kde using hmise\n",
    "    kde_factor = h_mise_list[i]/np.std(obs_sample_full[0:N],ddof=1) # divide to get correct h\n",
    "    hmise_kde = sps.gaussian_kde(this_sample,bw_method=kde_factor)\n",
    "    this_kde_dict['HMISE'].append(hmise_kde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codeblock gets the integrated squared error (ISE) for different sample sizes.\n",
    "\n",
    "However, we want to compute the mean $L^2$ error for a bunch of different samples to get the MISE (the expected ISE). This is expensive, so we do this in a different notebook [Expensive Computations](Expensive%20Computations.ipynb), and load the data here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute L2 errors\n",
    "# this_L2err_dict = {'Scott':[],'HMISE':[]}\n",
    "# for i,N in enumerate(N_list):\n",
    "#     this_err,tol = sf.L2_err_1D(obs_dist,this_kde_dict['Scott'][i],\n",
    "#                                 -10,25,quad_kwargs={'epsabs':1e-3})\n",
    "#     this_L2err_dict['Scott'].append(this_err)\n",
    "    \n",
    "#     this_err,tol = sf.L2_err_1D(obs_dist,this_kde_dict['HMISE'][i],\n",
    "#                                 -10,25,quad_kwargs={'epsabs':1e-3})\n",
    "#     this_L2err_dict['HMISE'].append(this_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the KDEs and ISE\n",
    "# fig_single_kde_converge, (ax1,ax2) = plt.subplots(1,2)\n",
    "# fig_single_kde_converge.set_figwidth(10)\n",
    "\n",
    "# i=3\n",
    "# ax1.plot(qx,obs_dist.pdf(qx),color='xkcd:black',ls='--',alpha=0.6)\n",
    "# ax1.plot(qx,this_kde_dict['Scott'][i](qx),\n",
    "#          label=\"Scott's Rule\")\n",
    "# ax1.plot(qx,this_kde_dict['HMISE'][i](qx),label='HMISE')\n",
    "# ax1.set_title('GKDE with $n={}$'.format(N_list[i]))\n",
    "# ax1.legend()\n",
    "\n",
    "# ax2.scatter(np.log(N_list),np.log(this_L2err_dict['Scott']))\n",
    "# ax2.scatter(np.log(N_list),np.log(this_L2err_dict['HMISE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try loading expensive kde error computations from\n",
    "# saved file\n",
    "try:\n",
    "    kde_error_data = np.load(EM.tri_peak_MISE_name+'.npz',allow_pickle=True)\n",
    "except FileNotFoundError as err:\n",
    "    print('File not found. Did you run the notebook '+\n",
    "          'with expensive computations?')\n",
    "    raise err\n",
    "    \n",
    "# check what keys are in the file\n",
    "print(kde_error_data.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the MISE using linear regression\n",
    "MISE_lines = {}\n",
    "these_kde_keys = ['ScottMISE', 'OptimalMISE']\n",
    "limit_B = len(kde_error_data['ScottMISE'])\n",
    "for label in these_kde_keys:\n",
    "    # take logarithms to put on log-log-scale\n",
    "    x_n = np.log(N_list*limit_B) # to agree with the dimesnions of data\n",
    "    y_err = np.log(kde_error_data[label].reshape(-1,))\n",
    "    this_MISE_line = np.polynomial.polynomial.Polynomial.fit(x_n,y_err,1)\n",
    "    MISE_lines[label] = this_MISE_line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows a GKDE and the Convergence Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the KDEs and ISE\n",
    "fig_kde_converge, (ax1,ax2) = plt.subplots(1,2)\n",
    "fig_kde_converge.set_figwidth(10)\n",
    "fig_kde_converge.set_figheight(4)\n",
    "\n",
    "\n",
    "i=3\n",
    "ax1.plot(qx,obs_dist.pdf(qx),color='xkcd:black',ls='--',alpha=0.7,\n",
    "         label='Target')\n",
    "ax1.plot(qx,this_kde_dict['Scott'][i](qx),\n",
    "         label=\"Scott's Rule\")\n",
    "ax1.plot(qx,this_kde_dict['HMISE'][i](qx),label='HMISE')\n",
    "ax1.set_title('GKDE with $n={}$'.format(N_list[i]))\n",
    "ax1.legend()\n",
    "\n",
    "line_labels = [\"Scott's\", \"HMISE\"]\n",
    "ax2.set_title('Convergence Rate in $L^2$')\n",
    "for i,label in enumerate(these_kde_keys):\n",
    "    # take logarithms to put on log-log-scale\n",
    "    x_n = np.log(N_list) # to agree with the dimesnions of data\n",
    "    y_n = MISE_lines[label](x_n)\n",
    "    y_var = np.std(np.log(kde_error_data[label]),ddof=1,axis=0)\n",
    "    \n",
    "    midpoint = (x_n[-1]+x_n[0])/2\n",
    "    y_label = MISE_lines[label](midpoint)\n",
    "    y_label += 0.5 if i ==0 else -0.5\n",
    "    ax2.annotate(\"$m={:0.2}$\".format(MISE_lines[label].coef[1]),\n",
    "                 xy = (midpoint,y_label), ha='right')\n",
    "     \n",
    "    ax2.errorbar(x_n,y_n, yerr = y_var,marker='o',\n",
    "                barsabove=True,capsize=4,label=line_labels[i])\n",
    "\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('Log Sample Size $n$')\n",
    "ax2.set_ylabel('Log $L^2$ Error')\n",
    "\n",
    "fig_kde_converge.tight_layout()\n",
    "# savefig\n",
    "this_fig_title = 'fig_kde_converge.png'\n",
    "fig_all_master[this_fig_title] = fig_kde_converge\n",
    "\n",
    "# # save just this fig\n",
    "# fig_kde_converge.savefig('../'+this_fig_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** The expected convergence rate may be estimated poorly for a small sample of $L^2$ errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Intervals for KDE\n",
    "\n",
    "We compute the confidence intervals in two different ways.\n",
    "\n",
    "1. Using the \"plug-in\" method\n",
    "2. Using a bootstrap sample\n",
    "\n",
    "**Formula for the plug-in method:**\n",
    "\n",
    "\\begin{align}\n",
    "C_{1-\\alpha}(q) = \\left[\\ \\widetilde{\\pi}_n(q)-z_{1-\\frac{\\alpha}{2}}\\sqrt{\\frac{R(K)\\widetilde{\\pi}_n(q)}{nh^2}}\\,\\ \\widetilde{\\pi}_n(q)+z_{1-\\frac{\\alpha}{2}}\\sqrt{\\frac{R(K)\\widetilde{\\pi}_n(q)}{nh^2}}\\ \\right]\n",
    "\\end{align}\n",
    "\n",
    "Recall that $R(K)=(2\\sqrt{\\pi})^{-1}$ from earlier.\n",
    "\n",
    "**Strategy for the Bootstrap method:**\n",
    "\n",
    "This is very costly, so we again do this in the [Expensive Computation](Expensive%20Computation.ipynb) notebook and then load the data here.\n",
    "\n",
    "Essentially, we just compute a bunch of GKDEs, then take the lower and upper pointwise values associated with the 95% quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try loading expensive kde error computations from\n",
    "# saved file\n",
    "try:\n",
    "    kde_CI_data = np.load(EM.tri_peak_CI_name+'.npz',allow_pickle=True)\n",
    "except FileNotFoundError as err:\n",
    "    print('File not found. Did you run the notebook '+\n",
    "          'with expensive computations?')\n",
    "    raise err\n",
    "    \n",
    "# check what keys are in the file\n",
    "print(kde_CI_data.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_dist = 3\n",
    "this_h = h_mise_list[ex_dist]\n",
    "this_n = N_list[ex_dist]\n",
    "print('N, R(K), h: ', this_n,RK,this_h) # take values from earlier\n",
    "\n",
    "# CI value\n",
    "alphaCI = 0.05\n",
    "z_lower,z_upper = sps.norm.ppf(alphaCI/2),sps.norm.ppf(1-alphaCI/2)\n",
    "\n",
    "# the  kde values for this estimate\n",
    "this_kde_vals = this_kde_dict['HMISE'][ex_dist](qx)\n",
    "\n",
    "# compute the confidence intervals plug-in and bootstrap\n",
    "\n",
    "pointwiseCI = {'Plugin': None,'Bootstrap': None}\n",
    "for CI in pointwiseCI:\n",
    "    if CI=='Plugin':\n",
    "        error_term = z_lower*np.sqrt(RK*this_kde_vals/(this_n*this_h**2))\n",
    "        qy_lower = this_kde_vals - error_term\n",
    "        qy_upper = this_kde_vals + error_term\n",
    "        \n",
    "    else:\n",
    "        this_q = kde_CI_data['HMISECI']\n",
    "        deviations = np.abs(this_q - this_kde_vals)\n",
    "        dev_bound = np.quantile(deviations,q=1-alphaCI,axis=0) \n",
    "        qy_lower = this_kde_vals-dev_bound\n",
    "        qy_upper = this_kde_vals+dev_bound\n",
    "    \n",
    "    # save the quantiles\n",
    "    pointwiseCI[CI] = (qy_lower,qy_upper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the KDEs and ISE\n",
    "fig_kde_confidence, axes = plt.subplots(1,2)\n",
    "fig_kde_confidence.set_figwidth(10)\n",
    "fig_kde_confidence.set_figheight(4)\n",
    "\n",
    "\n",
    "for i,ax in enumerate(axes):\n",
    "    ax.plot(qx,obs_dist.pdf(qx),color='xkcd:black',ls='--',alpha=0.7,\n",
    "             label='Target')\n",
    "    ax.plot(qx,this_kde_dict['HMISE'][ex_dist](qx),label='GKDE',color='C1')\n",
    "    \n",
    "    label = 'Plugin' if i == 0 else 'Bootstrap'\n",
    "        \n",
    "    qy_lower,qy_upper = pointwiseCI[label] \n",
    "    ax.fill_between(qx,qy_lower,qy_upper,edgecolor='xkcd:red',\n",
    "                    facecolor='xkcd:yellow orange',alpha=0.5,zorder=2,\n",
    "                    label='{:0.0f}% CI'.format(100*(1-alphaCI)))\n",
    "    \n",
    "    ax.set_title('{}% CI with $n={}$ Obs.'.format(int(100*(1-alphaCI)),\n",
    "                                                  N_list[ex_dist]))\n",
    "    ax.legend()\n",
    "\n",
    "# # savefig\n",
    "this_fig_title = 'fig_kde_CI.png'\n",
    "fig_all_master[this_fig_title] = fig_kde_confidence\n",
    "\n",
    "# # save just this fig\n",
    "# fig_kde_confidence.savefig('../'+this_fig_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Mixture Model and Dirichlet Process Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the Bayesian Mixture Model and the DP Mixture model are defined as follows:\n",
    "\n",
    "**Bayesian Finite Mixture Model**\n",
    "\\begin{align}\n",
    "f(q) &= \\sum_{k=1}^K w_k N(\\mu_k,\\Sigma_k) \\\\[1.5ex]\n",
    "(\\mu_k,\\Sigma_k)&\\sim NIW(\\mu_0,\\kappa_0,\\nu_0,\\Psi_0) \\\\[1.5ex]\n",
    "(w_1,\\ldots,w_k) &\\sim Dirichlet(\\alpha_0)\n",
    "\\end{align}\n",
    "\n",
    "**Bayesian Finite Mixture Model**\n",
    "\\begin{align}\n",
    "f(q) &= \\sum_{k=1}^K w_k N(\\mu_k,\\Sigma_k) \\\\[1.5ex]\n",
    "(\\mu_k,\\Sigma_k)&\\sim NIW(\\mu_0,\\kappa_0,\\nu_0,\\Psi_0) \\\\[1.5ex]\n",
    "\\beta_k &\\sim Beta(1,\\alpha_0) \\\\\n",
    "\\forall k :\\quad w_k &= \\beta_k \\prod_{j=1}^{k-1}(1-\\beta_k)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `scikit-learn`'s BayesianGaussianMixture package for all computations here. We translate the key components described by the prior model above to sklearn's argument language:\n",
    "\n",
    "* $\\alpha_0$ = `weight_concentration_prior_`, for either the beta distribution in the case of DP or the dirichlet distribution in the case of the finite mixture. \n",
    "\n",
    "\n",
    "* `weight_concentration_` : is the posterior vector of parameter values for the weight distribution, either a $K$-vector $\\alpha_0+n_k$ for the finite case or a vector of pairs of parameters $(a_k,b_k)$ corresponding to the posterior values of the beta distribution for each $k$ in the inifinite Dirichlet case.\n",
    "\n",
    "\n",
    "* $\\nu_0$, $\\hat{\\nu}$ = `degrees_of_freedom_prior_` and the posterior value `degrees_of_freedom_`\n",
    "\n",
    "\n",
    "* $\\kappa_0$, $\\hat{\\kappa}$ = `mean_precision_prior_` and the posterior value `mean_precision_`\n",
    "\n",
    "\n",
    "* $\\Psi_0$ = `covariance_prior_`. Note that there is a typo in the documentation: this is the correct parameter for the *inverse* wishart distribution, consistent with our model (not the wishart distribution).\n",
    "\n",
    "\n",
    "* $\\hat{\\Sigma}_k$ = `covariances_` are the estimated posterior covariances for each component $k$ after fitting the model. \n",
    "  \n",
    "  * To obtain $\\hat{\\Psi}_k$, the posterior parameter for the inverse-wishart distributions, use $\\hat{\\Sigma}_k\\cdot \\hat{\\nu}$.\n",
    "\n",
    "\n",
    "* $\\mu_0$ = `mean_prior_` center for the means. All $\\mu_k$ have this same initial prior center, so $\\mu_0$ is vector of the dimension $d$.\n",
    "\n",
    "\n",
    "* $\\hat{\\mu}$ = `means_` a vector of the posterior means of each component $k$ of the mixture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the general arguments that will be passed to BGMM\n",
    "arg_dict = EM.tri_peak_BGMM_arg_dict\n",
    "print(arg_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BGM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Prior Distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the prior distribution\n",
    "# different for different examples\n",
    "this_K = 5\n",
    "arg_prior_dict_A = {'n_components': this_K,\n",
    "                'weight_concentration_prior_type': 'dirichlet_distribution',\n",
    "                'weight_concentration_prior': 1,\n",
    "                'mean_prior': np.atleast_1d(np.round(np.mean(obs_sample_full))),\n",
    "                'mean_precision_prior': 1, # kappa\n",
    "                'degrees_of_freedom_prior': 1, # nu\n",
    "                'covariance_prior': np.atleast_2d(np.round(np.cov(obs_sample_full))) # psi\n",
    "                   }\n",
    "\n",
    "arg_prior_dict_B = arg_prior_dict_A.copy()\n",
    "arg_prior_dict_B['weight_concentration_prior'] = 20\n",
    "print('Prior Mean = ',arg_prior_dict_A['mean_prior'])\n",
    "print('Prior Cov. = ', arg_prior_dict_A['covariance_prior'])\n",
    "print('Prior Std. Dev. = ', np.sqrt(arg_prior_dict_A['covariance_prior']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two Bayesian prior models\n",
    "BGMM_A = BayesianGaussianMixture(**arg_prior_dict_A,**arg_dict)\n",
    "BGMM_B = BayesianGaussianMixture(**arg_prior_dict_B,**arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample for this example\n",
    "this_N = EM.tri_peak_CI_sample_size\n",
    "this_sample = obs_sample_full[0:this_N]\n",
    "\n",
    "# fit the BGMMs to this sample\n",
    "BGMM_A.fit(this_sample.reshape(-1,1))\n",
    "BGMM_B.fit(this_sample.reshape(-1,1))\n",
    "print('Model A: Converged? ', BGMM_A.converged_)\n",
    "print('Model B: Converged? ', BGMM_B.converged_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just look at the fits for reference\n",
    "\n",
    "# plt.plot(qx,sf.eval_pdf(qx,this_kde_dict['Scott'][3]))\n",
    "# plt.plot(qx,sf.eval_pdf(qx,BGMM_A),label='alpha={}'.format(BGMM_A.weight_concentration_prior))\n",
    "# plt.plot(qx,sf.eval_pdf(qx,BGMM_B),label='alpha={}'.format(BGMM_B.weight_concentration_prior))\n",
    "# plt.hist(this_sample,density=True,edgecolor='k',color='xkcd:sky blue', alpha=0.25)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # check components for reference\n",
    "# this_model_ref = BGMM_A\n",
    "# for k in np.arange(this_model_ref.n_components):\n",
    "#     rel_w = this_model_ref.weights_[k]\n",
    "#     mean, sig2 = this_model_ref.means_[k],np.squeeze(this_model_ref.covariances_[k])\n",
    "#     plt.plot(qx,rel_w*sps.norm.pdf(qx,loc=mean, scale=np.sqrt(sig2)),label=k,alpha=1)\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save prior parameters for forward sampling\n",
    "this_prior_list = {'priorA': {},\n",
    "                   'priorB': {}}\n",
    "this_prior_list['priorA']['Fmodel'] = sf.Forward_BGM_Model(BGMM_A,prior=True)\n",
    "this_prior_list['priorB']['Fmodel'] = sf.Forward_BGM_Model(BGMM_B,prior=True)\n",
    "\n",
    "# get some samples of parameters\n",
    "this_M = 8\n",
    "for key in this_prior_list:\n",
    "    this_prior_list[key]['param_sample'] = this_prior_list[key]['Fmodel'].rvs(this_M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the priors sampled\n",
    "fig_BGM_priors, axes = plt.subplots(2,2)\n",
    "fig_BGM_priors.set_figheight(8)\n",
    "fig_BGM_priors.set_figwidth(8)\n",
    "for i,key in enumerate(this_prior_list):\n",
    "    # compute the ys of the sample\n",
    "    # batch GMM returns M x (evalx) array\n",
    "    prior_ys = sf.batch_GMM_pdf(qx,this_prior_list[key]['param_sample'])\n",
    "    \n",
    "    # plot expected prior\n",
    "    qy = this_prior_list[key]['Fmodel'].pdf(qx)\n",
    "    axes[i][0].plot(qx,qy,label='Expected')\n",
    "    \n",
    "    # plot sampled prior\n",
    "    for k,qy in enumerate(prior_ys):\n",
    "        if k==1:\n",
    "            axes[i][0].plot(qx,qy,color='gray',alpha=0.75,label='Sampled')\n",
    "        else:\n",
    "            axes[i][0].plot(qx,qy,color='gray',alpha=0.65)\n",
    "    axes[i][0].legend()\n",
    "    \n",
    "        \n",
    "    # bar graph of the weights\n",
    "    weight_bars = np.arange(this_prior_list[key]['Fmodel'].K)+1\n",
    "    axes[i][1].bar(weight_bars,this_prior_list[key]['Fmodel'].weight_dist.mean(),\n",
    "                   yerr=np.sqrt(this_prior_list[key]['Fmodel'].weight_dist.var()),\n",
    "                  error_kw={'capsize': 4})\n",
    "    # can be used to check reliability of sampling weights is same as expected\n",
    "#     axes[i][1].bar(weight_bars,this_prior_list[key]['param_sample']['weight'].mean(axis=0))\n",
    "\n",
    "    # set titles\n",
    "    this_alpha = this_prior_list[key]['Fmodel'].weight_dist.alpha\n",
    "    axes[i][0].set_title('Prior, $\\\\alpha={}$'.format(this_alpha[0]))\n",
    "    axes[i][1].set_title('Exp. Weights, $\\\\alpha={}$'.format(this_alpha[0]))\n",
    "\n",
    "# labels\n",
    "axes[1,0].set_xlabel('$q$')\n",
    "axes[1,1].set_xlabel('Component $k$')\n",
    "# use ylim of first plot\n",
    "axes[1,1].set_ylim(axes[0,1].get_ylim())\n",
    "\n",
    "fig_BGM_priors.tight_layout()\n",
    "\n",
    "# # savefig\n",
    "this_fig_title = 'fig_BGM_prior.png'\n",
    "fig_all_master[this_fig_title] = fig_BGM_priors\n",
    "\n",
    "# # save just this fig\n",
    "# fig_BGM_priors.savefig('../'+this_fig_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute forward samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prior parameters for forward sampling\n",
    "this_post_list = {'postA': {'model': BGMM_A },\n",
    "                   'postB': {'model': BGMM_B}}\n",
    "this_post_list['postA']['Fmodel'] = sf.Forward_BGM_Model(BGMM_A)\n",
    "this_post_list['postB']['Fmodel'] = sf.Forward_BGM_Model(BGMM_B)\n",
    "\n",
    "# get some samples of parameters\n",
    "this_M = 500\n",
    "for key in this_post_list:\n",
    "    this_post_list[key]['param_sample'] = this_post_list[key]['Fmodel'].rvs(this_M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to plot the component distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each model, get the component distributions\n",
    "for key in this_post_list:\n",
    "    # for this model\n",
    "    this_model = this_post_list[key]['model']\n",
    "    this_K = this_model.n_components\n",
    "    \n",
    "    # compute the pdfs\n",
    "    these_pdf_outs = np.zeros([this_K,len(qx)])\n",
    "    for k in np.arange(this_K): \n",
    "        this_mean = this_model.means_[k] \n",
    "        this_cov = this_model.covariances_[k]\n",
    "        this_weight = this_model.weights_[k]\n",
    "        these_pdf_outs[k,:] = this_weight*sps.multivariate_normal.pdf(qx,\n",
    "                                         mean = this_mean, cov = this_cov)\n",
    "    \n",
    "    # save the pdf outs\n",
    "    this_post_list[key]['comp_pdfs'] = these_pdf_outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the post sampled\n",
    "fig_BGM_post, axes = plt.subplots(2,2)\n",
    "fig_BGM_post.set_figheight(8)\n",
    "fig_BGM_post.set_figwidth(10)\n",
    "for i,key in enumerate(this_post_list):\n",
    "    # plot target \n",
    "    axes[i][0].plot(qx,sf.eval_pdf(qx,obs_dist), ls='--', label='Target',\n",
    "                    color='k',alpha=0.4,zorder=-1)\n",
    "    \n",
    "    # plot expected posterior\n",
    "    qy = sf.eval_pdf(qx,this_post_list[key]['model'])\n",
    "    axes[i][0].plot(qx,qy,label='Expected',zorder=0,linewidth=2)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # bar graph of the weights\n",
    "    this_K = this_post_list[key]['Fmodel'].K\n",
    "    weight_bars = np.arange(this_K)+1\n",
    "    mean_weights = this_post_list[key]['Fmodel'].weight_dist.mean()\n",
    "    var_weights = this_post_list[key]['Fmodel'].weight_dist.var()\n",
    "    \n",
    "    # order K components for color niceness\n",
    "    large_ind_to_small = np.flip(np.argsort(mean_weights))\n",
    "    colorlist = ['xkcd:orangered','xkcd:crimson','xkcd:orchid']+['C0']*(this_K-3)\n",
    "    ordered_colorlist = np.empty(this_K,dtype='object')\n",
    "    for j,val in enumerate(ordered_colorlist):\n",
    "        ordered_colorlist[large_ind_to_small[j]] = colorlist[j]\n",
    "#     print(mean_weights)\n",
    "#     print(large_ind_to_small)\n",
    "#     print(ordered_colorlist)\n",
    "    \n",
    "    # bar plot\n",
    "    axes[i][1].bar(weight_bars,mean_weights, yerr=np.sqrt(var_weights),\n",
    "                  error_kw={'capsize': 4},edgecolor=ordered_colorlist,\n",
    "                   linewidth=2)\n",
    "\n",
    "    # plot the components\n",
    "    for k,y_pdf in enumerate(this_post_list[key]['comp_pdfs']):\n",
    "        if mean_weights[k] >= mean_weights[large_ind_to_small[2]]:\n",
    "            axes[i][0].plot(qx,y_pdf, color=ordered_colorlist[k],ls='-.',\n",
    "                            linewidth=2)\n",
    "        else:\n",
    "            axes[i][0].plot(qx,y_pdf, color=colorlist[-1],ls='--')\n",
    "    axes[i][0].legend()\n",
    "\n",
    "    # set titles:\n",
    "    this_alpha = [1,20]\n",
    "    axes[i][0].set_title('Posterior, $\\\\alpha_0={}$'.format(this_alpha[i]))\n",
    "    axes[i][1].set_title('Exp. Weights, $\\\\alpha_0={}$'.format(this_alpha[i]))\n",
    "\n",
    "# labels\n",
    "axes[1,0].set_xlabel('$q$')\n",
    "axes[1,1].set_xlabel('Component $k$')\n",
    "# use ylim of first plot\n",
    "axes[1,1].set_ylim(axes[0,1].get_ylim())\n",
    "\n",
    "\n",
    "# # savefig\n",
    "fig_BGM_post.tight_layout()\n",
    "this_fig_title = 'fig_BGM_post.png'\n",
    "fig_all_master[this_fig_title] = fig_BGM_post\n",
    "\n",
    "# # save just this fig\n",
    "# fig_BGM_post.savefig('../'+this_fig_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the $L^2$ distance between the two distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.L2_err_1D(BGMM_A,BGMM_B,-10,25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that this is not the best estimate. This is due to the BGMM oversmoothing and is similar to when the bandwidth is chosen to be too large.\n",
    "\n",
    "We can improve our estimate by utilizing prior knowledge of the data-generating distribution.\n",
    "\n",
    "Specifically, we use the standard deviation of the smallest component in our  mixture to set an appropriate our \"window\" length.\n",
    "\n",
    "When we do this, we adjust the prior precision parameter to be less than 1 to reflect that we expect the means to vary a lot more than the given covariances. That is:\n",
    "\n",
    "\\begin{align}\n",
    "Cov(\\mu_i) &> E(\\Sigma_i) \\\\\n",
    "Cov(\\mu_i) &= (1/\\kappa_0) E(\\Sigma_i) \\\\\n",
    "\\Rightarrow \\kappa_0 &< 1.\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure I am using the same sample\n",
    "this_N = EM.tri_peak_CI_sample_size\n",
    "this_sample = obs_sample_full[0:this_N]\n",
    "print(np.cov(this_sample))\n",
    "\n",
    "# get a window size that produces a better fit\n",
    "var_min = np.min([EM.distA.var(),EM.distB.var(),EM.distC.var()])\n",
    "window = np.sqrt(var_min)\n",
    "print(window)\n",
    "\n",
    "# scale precision parameter so that variance of the means is \n",
    "# variance of the sample\n",
    "this_kappa0 = window**2/np.cov(this_sample)\n",
    "print(this_kappa0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new prior with adjusted window\n",
    "arg_prior_dict_C = arg_prior_dict_A.copy()\n",
    "arg_prior_dict_C['covariance_prior'] = np.atleast_2d(window)\n",
    "arg_prior_dict_C['mean_precision_prior'] = this_kappa0\n",
    "arg_prior_dict_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # change number of random starts\n",
    "# arg_dict_C = arg_dict.copy()\n",
    "# arg_dict_C['n_init'] = 5\n",
    "# arg_dict_C['tol'] = 1e-3\n",
    "# arg_dict_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the new mixture\n",
    "BGMM_C = BayesianGaussianMixture(**arg_prior_dict_C,**arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure I am using the same sample\n",
    "this_N = EM.tri_peak_CI_sample_size\n",
    "this_sample = obs_sample_full[0:this_N]\n",
    "\n",
    "BGMM_C.fit(this_sample.reshape(-1,1))\n",
    "\n",
    "print('Model C: Converged? ', BGMM_C.converged_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just look at the fits for reference\n",
    "# plt.plot(qx,sf.eval_pdf(qx,this_kde_dict['Scott'][3]))\n",
    "# plt.plot(qx,sf.eval_pdf(qx,BGMM_C),label='BGMM C')\n",
    "# plt.hist(this_sample,density=True,edgecolor='k',color='xkcd:sky blue', alpha=0.25)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check components for reference\n",
    "# this_model_ref = BGMM_C\n",
    "# for k in np.arange(this_model_ref.n_components):\n",
    "#     rel_w = this_model_ref.weights_[k]\n",
    "#     mean, sig2 = this_model_ref.means_[k],np.squeeze(this_model_ref.covariances_[k])\n",
    "#     plt.plot(qx,rel_w*sps.norm.pdf(qx,loc=mean, scale=np.sqrt(sig2)),label=k,alpha=1)\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_BGMM_kde_compare, axes =plt.subplots(1,2)\n",
    "fig_BGMM_kde_compare.set_figwidth(10)\n",
    "\n",
    "for i,ax in enumerate(axes):\n",
    "    ax.plot(qx,sf.eval_pdf(qx,obs_dist), ls='--', label='Target',\n",
    "                    color='k',alpha=0.4,zorder=-1)\n",
    "\n",
    "    if i ==0:\n",
    "        ax.plot(qx,this_kde_dict['Scott'][3](qx),label=\"GKDE: $h_{Scott}$\")\n",
    "        ax.plot(qx,sf.eval_pdf(qx,BGMM_A),\n",
    "                 label=\"BGMM: $\\Psi\\\\approx s_q^2$\")\n",
    "        ax.set_title('Uninformed Parameter Choice')\n",
    "        \n",
    "    else:\n",
    "        ax.plot(qx,this_kde_dict['HMISE'][3](qx),label='GKDE: $h_{HMISE}$')\n",
    "        ax.plot(qx,sf.eval_pdf(qx,BGMM_C),\n",
    "                 label=\"BGMM: $\\Psi\\\\approx \\sigma_{min}^2$\")\n",
    "        ax.set_title('Informed Parameter Choice')\n",
    "        \n",
    "    ax.legend()\n",
    "    ax.set_xlabel('$q$')\n",
    "\n",
    "# # savefig\n",
    "this_fig_title = 'fig_BGM_KDE_compare.png'\n",
    "fig_all_master[this_fig_title] = fig_BGMM_kde_compare\n",
    "\n",
    "# save just this fig\n",
    "# fig_BGMM_kde_compare.savefig('../'+this_fig_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the confidence interval for the density estimate.\n",
    "\n",
    "We also show that the BGMM models converge. For this, we take a sample of BGMM models and then compute the $L^2$ error. This is in the [Expensive Computations](Expensive%20Computations.ipynb) notebook: we just import the results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the small window to the pdf list\n",
    "this_post_list['postC'] = {'model': BGMM_C}\n",
    "this_post_list['postC']['Fmodel'] = sf.Forward_BGM_Model(BGMM_C)\n",
    "\n",
    "# get some samples of parameters\n",
    "this_post_list['postC']['param_sample'] = this_post_list['postC']['Fmodel'].rvs(this_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sampled post use for quantiles\n",
    "# compute the ys of the sample\n",
    "# batch GMM returns M x (evalx) array\n",
    "for key in this_post_list:\n",
    "    post_ys = sf.batch_GMM_pdf(qx,this_post_list[key]['param_sample'])\n",
    "    this_post_list[key]['sampled_post_ys'] = post_ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try loading expensive kde error computations from\n",
    "# saved file\n",
    "try:\n",
    "    BGMM_L2err_data = np.load(EM.tri_peak_BGM_name+'.npz',allow_pickle=True)\n",
    "except FileNotFoundError as err:\n",
    "    print('File not found. Did you run the notebook '+\n",
    "          'with expensive computations?')\n",
    "    raise err\n",
    "    \n",
    "# check what keys are in the file\n",
    "print(BGMM_L2err_data.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(BGMM_L2err_data['BGMM_C_L2_err'] - BGMM_L2err_data['DPMM_C_L2_err'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the KDEs and ISE\n",
    "fig_BGM_confidence, axes = plt.subplots(1,2)\n",
    "fig_BGM_confidence.set_figwidth(10)\n",
    "fig_BGM_confidence.set_figheight(4)\n",
    "\n",
    "# define confidence level\n",
    "this_CIa = 0.05\n",
    "\n",
    "these_BGMM_errs = ['BGMM_A_L2_err','BGMM_C_L2_err']\n",
    "for i,ax in enumerate(axes):\n",
    "    if i == 0:\n",
    "        axes[i].plot(qx,obs_dist.pdf(qx),color='xkcd:black',ls='--',alpha=0.7,\n",
    "                 label='Target')\n",
    "        qy = sf.eval_pdf(qx,this_post_list['postC']['model'])\n",
    "        axes[i].plot(qx,qy,label='Exp. Posterior',color='C1')\n",
    "\n",
    "        post_ys = this_post_list['postC']['sampled_post_ys']\n",
    "        qy_lower = np.quantile(post_ys,q=this_CIa/2,axis=0)\n",
    "        qy_upper = np.quantile(post_ys,q=1-this_CIa/2,axis=0)\n",
    "        axes[i].fill_between(qx,qy_lower,qy_upper,edgecolor='xkcd:red',\n",
    "                        facecolor='xkcd:yellow orange',alpha=0.5,zorder=2,\n",
    "                        label='{:0.0f}% CI'.format(100*(1-alphaCI)))\n",
    "\n",
    "        axes[i].set_title('{}% CI with $n={}$ Obs.'.format(int(100*(1-alphaCI)),\n",
    "                                                      N_list[ex_dist]))\n",
    "        axes[i].legend()\n",
    "        axes[i].set_xlabel('$q$')\n",
    "    else:\n",
    "        line_labels = [\"BGMM: $\\Psi\\\\approx s_q^2$\", \n",
    "                       \"BGMM: $\\Psi\\\\approx \\sigma_{min}^2$\"]\n",
    "        ax.set_title('Convergence Rate in $L^2$')\n",
    "        for i,label in enumerate(these_BGMM_errs):\n",
    "            # take logarithms to put on log-log-scale\n",
    "            print(label)\n",
    "            x_n = np.log(N_list) # to agree with the dimesnions of data\n",
    "            y_n = np.mean(np.log(BGMM_L2err_data[label]),axis=0)\n",
    "            y_var = np.std(np.log(BGMM_L2err_data[label]),ddof=1,axis=0)\n",
    "\n",
    "            midpoint = (x_n[-1]+x_n[0])/2\n",
    "#             y_label = MISE_lines[label](midpoint)\n",
    "#             y_label += 0.5 if i ==0 else -0.5\n",
    "#             ax.annotate(\"$m={:0.2}$\".format(MISE_lines[label].coef[1]),\n",
    "#                          xy = (midpoint,y_label), ha='right')\n",
    "\n",
    "            ax.errorbar(x_n,y_n, yerr = y_var,marker='o',\n",
    "                        barsabove=True,capsize=4,label=line_labels[i])\n",
    "\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('Log Sample Size $n$')\n",
    "        ax.set_ylabel('Log $L^2$ Error')\n",
    "\n",
    "# # savefig\n",
    "fig_BGM_confidence.tight_layout()\n",
    "this_fig_title = 'fig_BGM_confidence_L2.png'\n",
    "fig_all_master[this_fig_title] = fig_BGM_confidence\n",
    "\n",
    "# # save just this fig\n",
    "# fig_BGM_confidence.savefig('../'+this_fig_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet Process Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior DPMM\n",
    "\n",
    "First we visualize several prior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the general arguments that will be passed to DPMM\n",
    "arg_dict = EM.tri_peak_BGMM_arg_dict\n",
    "print(arg_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the key argument change is the weight concentration prior type (changed to Dirichlet process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up prior for this example\n",
    "this_K = 35\n",
    "\n",
    "# make sure I am using the same sample\n",
    "this_N = EM.tri_peak_CI_sample_size\n",
    "this_sample = obs_sample_full[0:this_N]\n",
    "\n",
    "# prior dictionary\n",
    "arg_prior_dict_DP = {'n_components': this_K,\n",
    "                'weight_concentration_prior_type': 'dirichlet_process',\n",
    "                'weight_concentration_prior': 1,\n",
    "                'mean_prior': np.atleast_1d(np.round(np.mean(this_sample))),\n",
    "                'mean_precision_prior': 1, # kappa\n",
    "                'degrees_of_freedom_prior': 1, # nu\n",
    "                'covariance_prior': np.atleast_2d(np.round(np.cov(this_sample))) # psi\n",
    "                   }\n",
    "\n",
    "# prior dict with large alpha\n",
    "arg_prior_dict_DP_B = arg_prior_dict_DP.copy()\n",
    "arg_prior_dict_DP_B['weight_concentration_prior'] = 5 # alpha = 5\n",
    "\n",
    "print(arg_prior_dict_DP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the DPMM\n",
    "DPMM_A = BayesianGaussianMixture(**arg_prior_dict_DP,**arg_dict)\n",
    "\n",
    "# dpmm with large alpha\n",
    "DPMM_B = BayesianGaussianMixture(**arg_prior_dict_DP_B,**arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the DPMM\n",
    "DPMM_A.fit(this_sample.reshape(-1,1))\n",
    "DPMM_B.fit(this_sample.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just look at the fits for reference\n",
    "# plt.plot(qx,sf.eval_pdf(qx,this_kde_dict['Scott'][3]))\n",
    "# plt.plot(qx,sf.eval_pdf(qx,DPMM_A),label='DPMM A')\n",
    "# plt.hist(this_sample,density=True,edgecolor='k',color='xkcd:sky blue', alpha=0.25)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check components for reference\n",
    "# this_model_ref = DPMM_A\n",
    "# for k in np.arange(this_model_ref.n_components):\n",
    "#     rel_w = this_model_ref.weights_[k]\n",
    "#     if rel_w > 0.01:\n",
    "#         mean, sig2 = this_model_ref.means_[k],np.squeeze(this_model_ref.covariances_[k])\n",
    "#         plt.plot(qx,rel_w*sps.norm.pdf(qx,loc=mean, scale=np.sqrt(sig2)),label=k,alpha=1)\n",
    "        \n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save prior parameters for forward sampling\n",
    "this_prior_list = {'priorA': {},\n",
    "                   'priorB': {}}\n",
    "this_prior_list['priorA']['Fmodel'] = sf.Forward_BGM_Model(DPMM_A,prior=True)\n",
    "this_prior_list['priorB']['Fmodel'] = sf.Forward_BGM_Model(DPMM_B,prior=True)\n",
    "\n",
    "# get some samples of parameters\n",
    "this_M = 1000\n",
    "for key in this_prior_list:\n",
    "    this_prior_list[key]['param_sample'] = this_prior_list[key]['Fmodel'].rvs(this_M)\n",
    "    prior_ys = sf.batch_GMM_pdf(qx,this_prior_list[key]['param_sample'])\n",
    "    this_prior_list[key]['priorys'] = prior_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the priors sampled\n",
    "fig_DPMM_priors, axes = plt.subplots(2,2)\n",
    "fig_DPMM_priors.set_figheight(8)\n",
    "fig_DPMM_priors.set_figwidth(8)\n",
    "\n",
    "this_alpha = [1,5]\n",
    "trunc_K_plot = sum(np.mean(this_prior_list['priorB']['param_sample']['weight'],axis=0)>0.01)\n",
    "for i,key in enumerate(this_prior_list):\n",
    "    # compute the ys of the sample\n",
    "    # batch GMM returns M x (evalx) array\n",
    "    prior_ys_subset = this_prior_list[key]['priorys'][0:5]\n",
    "    \n",
    "    # plot expected prior\n",
    "    qy = this_prior_list[key]['Fmodel'].pdf(qx)\n",
    "    axes[i][0].plot(qx,qy,label='Expected')\n",
    "    \n",
    "    # plot sampled prior\n",
    "    for k,qy in enumerate(prior_ys_subset):\n",
    "        if k==1:\n",
    "            axes[i][0].plot(qx,qy,color='gray',alpha=0.75,label='Sampled')\n",
    "        else:\n",
    "            axes[i][0].plot(qx,qy,color='gray',alpha=0.65)\n",
    "    axes[i][0].legend()\n",
    "    \n",
    "        \n",
    "    # bar graph of the weights\n",
    "    weight_bars = np.arange(this_prior_list[key]['Fmodel'].K)+1\n",
    "    height_bars = np.mean(this_prior_list[key]['param_sample']['weight'],axis=0)\n",
    "    height_std = np.sqrt(np.var(this_prior_list[key]['param_sample']['weight'],ddof=1,axis=0))\n",
    "    low_err = np.min(np.array([height_std,height_bars]),axis=0)\n",
    "    err_bars = np.array([low_err,height_std])\n",
    "    axes[i][1].bar(weight_bars[0:trunc_K_plot+1],height_bars[0:trunc_K_plot+1],\n",
    "                   yerr=err_bars[:,0:trunc_K_plot+1],\n",
    "                  error_kw={'capsize': 2,'linewidth':1})\n",
    "\n",
    "    # set titles\n",
    "    axes[i][0].set_title('Prior, $\\\\alpha={}$'.format(this_alpha[i]))\n",
    "    axes[i][1].set_title('Exp. Weights, $\\\\alpha={}$'.format(this_alpha[i]))\n",
    "\n",
    "# labels\n",
    "axes[1,0].set_xlabel('$q$')\n",
    "axes[1,1].set_xlabel('Component $k$')\n",
    "# use ylim of first plot\n",
    "axes[1,1].set_ylim(axes[0,1].get_ylim())\n",
    "\n",
    "# # savefig\n",
    "fig_DPMM_priors.tight_layout()\n",
    "this_fig_title = 'fig_DPMM_prior.png'\n",
    "fig_all_master[this_fig_title] = fig_DPMM_priors\n",
    "\n",
    "# # save just this fig\n",
    "# fig_DPMM_priors.savefig('../'+this_fig_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the histogram of truncated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(1-np.cumsum(this_prior_list[key]['param_sample']['weight'],axis=1)>0.01,axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-np.cumsum(this_prior_list[key]['param_sample']['weight'],axis=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_DPMM_hist_trunc, axes = plt.subplots(1,2)\n",
    "fig_DPMM_hist_trunc.set_figwidth(8)\n",
    "# fig_DPMM_hist_trunc.set_figwidth(3/4*6)\n",
    "\n",
    "for i,key in enumerate(this_prior_list):\n",
    "    # counts the number of components before 99% of the weight probability reached\n",
    "    K_trunc = np.sum(1-np.cumsum(this_prior_list[key]['param_sample']['weight'],\n",
    "                                 axis=1)>0.01,axis=1)\n",
    "    \n",
    "    axes[i].hist(K_trunc+1,edgecolor='k')\n",
    "    axes[i].set_xlabel('Truncatation $K_{\\\\delta}$')\n",
    "    axes[i].set_title('DPMM Components, $\\\\alpha={}$'.format(this_alpha[i]))\n",
    "\n",
    "# # savefig\n",
    "this_fig_title = 'fig_DPMM_truncation.png'\n",
    "fig_all_master[this_fig_title] = fig_DPMM_hist_trunc\n",
    "\n",
    "# # save just this fig\n",
    "# fig_DPMM_hist_trunc.savefig('../'+this_fig_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DPMM Posterior\n",
    "\n",
    "For the posterior plots here, we use the better covariance estimate for $\\Psi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic params dictionary\n",
    "print(arg_dict)\n",
    "print()\n",
    "\n",
    "# same as prior C for BGMM but with Dirichlet Process\n",
    "arg_prior_dict_DP_C = arg_prior_dict_C.copy()\n",
    "arg_prior_dict_DP_C['n_components'] = 30\n",
    "arg_prior_dict_DP_C['weight_concentration_prior_type'] = 'dirichlet_process'\n",
    "print(arg_prior_dict_DP_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "DPMM_C = BayesianGaussianMixture(**arg_prior_dict_DP_C,**arg_dict)\n",
    "\n",
    "# fit the model\n",
    "DPMM_C.fit(this_sample.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the expected posterior distribution and plot the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each model, get the component distributions\n",
    "this_post_list['post_DPMM'] = {'model': DPMM_C,\n",
    "                               'Fmodel': sf.Forward_BGM_Model(DPMM_C)}\n",
    "\n",
    "this_model = this_post_list['post_DPMM']['model']\n",
    "this_K = this_model.n_components\n",
    "\n",
    "# compute the pdfs\n",
    "key = 'post_DPMM'\n",
    "these_pdf_outs = np.zeros([this_K,len(qx)])\n",
    "for k in np.arange(this_K): \n",
    "    this_mean = this_model.means_[k] \n",
    "    this_cov = this_model.covariances_[k]\n",
    "    this_weight = this_model.weights_[k]\n",
    "    these_pdf_outs[k,:] = this_weight*sps.multivariate_normal.pdf(qx,\n",
    "                                     mean = this_mean, cov = this_cov)\n",
    "\n",
    "# save the pdf outs\n",
    "this_post_list[key]['comp_pdfs'] = these_pdf_outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the post sampled\n",
    "fig_DPMM_post, axes = plt.subplots(1,2)\n",
    "fig_DPMM_post.set_figwidth(10)\n",
    "\n",
    "key = 'post_DPMM'\n",
    "# plot target \n",
    "axes[0].plot(qx,sf.eval_pdf(qx,obs_dist), ls='--', label='Target',\n",
    "                color='k',alpha=0.4,zorder=-1)\n",
    "\n",
    "# plot expected posterior\n",
    "qy = sf.eval_pdf(qx,this_post_list[key]['model'])\n",
    "axes[0].plot(qx,qy,label='Expected',zorder=0,linewidth=2)\n",
    "\n",
    "\n",
    "# bar graph of the weights\n",
    "mean_weights = this_post_list[key]['Fmodel'].weight_dist.mean()\n",
    "\n",
    "# order K components: keep the top 5\n",
    "topX = 5\n",
    "large_ind_to_small = np.flip(np.argsort(mean_weights))\n",
    "print(large_ind_to_small[0:topX])\n",
    "colorlist = ['xkcd:orangered','xkcd:crimson','xkcd:orchid']+['C0']*(topX-3)\n",
    "weight_bars = np.arange(topX)+1\n",
    "\n",
    "# get forward sample to compute weight variances\n",
    "weight_std = np.sqrt(np.var(this_post_list['post_DPMM']['Fmodel'].weight_dist.rvs(500),\n",
    "                    ddof=1,axis=0))\n",
    "low_err = np.min(np.array([weight_std,mean_weights]),axis=0)\n",
    "err_bars = np.array([low_err,weight_std])\n",
    "\n",
    "# bar plot\n",
    "axes[1].bar(weight_bars,mean_weights[large_ind_to_small[0:topX]],edgecolor=colorlist,\n",
    "               linewidth=2,tick_label=large_ind_to_small[0:topX]+1,\n",
    "                yerr=err_bars[:,large_ind_to_small[0:topX]],\n",
    "                  error_kw={'capsize': 4}\n",
    "               )\n",
    "\n",
    "# plot the components\n",
    "for k,y_pdf in enumerate(this_post_list[key]['comp_pdfs'][large_ind_to_small[0:topX]]):\n",
    "#     print(k,mean_weights[k])\n",
    "    axes[0].plot(qx,y_pdf, color=colorlist[k],ls='-.',\n",
    "                    linewidth=2)\n",
    "\n",
    "axes[0].legend()\n",
    "\n",
    "# set titles:\n",
    "this_alpha = [1]\n",
    "axes[0].set_title('Posterior DPMM, $\\\\alpha={}$'.format(this_alpha[0]))\n",
    "axes[1].set_title('Posterior Average Weights')\n",
    "\n",
    "# labels\n",
    "axes[0].set_xlabel('$q$')\n",
    "axes[1].set_xlabel('Component # $k$')\n",
    "# use ylim of first plot\n",
    "\n",
    "# # savefig\n",
    "this_fig_title = 'fig_DPMM_post.png'\n",
    "fig_all_master[this_fig_title] = fig_DPMM_post\n",
    "\n",
    "# # save just this fig\n",
    "# fig_DPMM_post.savefig('../'+this_fig_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credible Interval of the DPMM and Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next plot we show the DPMM Credible intervals and also show the convergence rates. \n",
    "\n",
    "The $L^2$ convergence rate is loaded from the same file as the BGMM $L^2$ error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the posterior and save parameters\n",
    "key = 'post_DPMM'\n",
    "this_M = 500\n",
    "this_post_list[key]['param_sample'] = this_post_list[key]['Fmodel'].rvs(this_M)\n",
    "\n",
    "# compute the posterior sampled pdfs\n",
    "post_ys = sf.batch_GMM_pdf(qx,this_post_list[key]['param_sample'])\n",
    "this_post_list[key]['sampled_post_ys'] = post_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the file is loaded\n",
    "BGMM_L2err_data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the KDEs and ISE\n",
    "fig_DPMM_confidence, axes = plt.subplots(1,2)\n",
    "fig_DPMM_confidence.set_figwidth(12)\n",
    "\n",
    "# define confidence level\n",
    "this_CIa = 0.05\n",
    "\n",
    "key = 'post_DPMM'\n",
    "these_DPMM_errs = ['BGMM_C_L2_err','DPMM_C_L2_err']\n",
    "for i,ax in enumerate(axes):\n",
    "    if i == 0:\n",
    "        axes[i].plot(qx,obs_dist.pdf(qx),color='xkcd:black',ls='--',alpha=0.7,\n",
    "                 label='Target')\n",
    "        qy = sf.eval_pdf(qx,this_post_list[key]['model'])\n",
    "        axes[i].plot(qx,qy,label='Exp. Posterior',color='C1')\n",
    "\n",
    "        post_ys = this_post_list[key]['sampled_post_ys']\n",
    "        qy_lower = np.quantile(post_ys,q=this_CIa/2,axis=0)\n",
    "        qy_upper = np.quantile(post_ys,q=1-this_CIa/2,axis=0)\n",
    "        axes[i].fill_between(qx,qy_lower,qy_upper,edgecolor='xkcd:red',\n",
    "                        facecolor='xkcd:yellow orange',alpha=0.5,zorder=2,\n",
    "                        label='{:0.0f}% CI'.format(100*(1-alphaCI)))\n",
    "\n",
    "        axes[i].set_title('{}% CI with $n={}$ Obs.'.format(int(100*(1-alphaCI)),\n",
    "                                                      N_list[ex_dist]))\n",
    "        axes[i].legend()\n",
    "        axes[i].set_xlabel('$q$')\n",
    "    else:\n",
    "        line_labels = [\"BGMM\", \n",
    "                       \"DPMM\"]\n",
    "        ax.set_title('Convergence Rate in $L^2$')\n",
    "        for i,label in enumerate(these_DPMM_errs):\n",
    "            print(label)\n",
    "            # take logarithms to put on log-log-scale\n",
    "            x_n = np.log(N_list) # to agree with the dimesnions of data\n",
    "            y_n = np.mean(np.log(BGMM_L2err_data[label]),axis=0)\n",
    "            y_var = np.std(np.log(BGMM_L2err_data[label]),ddof=1,axis=0)\n",
    "\n",
    "            midpoint = (x_n[-1]+x_n[0])/2\n",
    "#             y_label = MISE_lines[label](midpoint)\n",
    "#             y_label += 0.5 if i ==0 else -0.5\n",
    "#             ax.annotate(\"$m={:0.2}$\".format(MISE_lines[label].coef[1]),\n",
    "#                          xy = (midpoint,y_label), ha='right')\n",
    "\n",
    "            ax.errorbar(x_n,y_n, yerr = y_var,marker='o',\n",
    "                        barsabove=True,capsize=4,label=line_labels[i])\n",
    "\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('Log Sample Size $n$')\n",
    "        ax.set_ylabel('Log $L^2$ Error')\n",
    "\n",
    "# # savefig\n",
    "this_fig_title = 'fig_DPMM_confidence_L2.png'\n",
    "fig_all_master[this_fig_title] = fig_DPMM_confidence\n",
    "\n",
    "# #save just this fig\n",
    "# fig_DPMM_confidence.savefig('../'+this_fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare DPMM to BGMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we make a BGMM with $K=30$ components so that we can compare it to the DPMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance based on the bandwidth\n",
    "psi_hmise = np.atleast_2d(h_mise_list[3]**2)\n",
    "\n",
    "# make a DPMM with new hmise \n",
    "arg_prior_dict_DP_D = arg_prior_dict_DP_C.copy()\n",
    "arg_prior_dict_DP_D['covariance_prior'] = psi_hmise\n",
    "print('DPMM D:')\n",
    "print(arg_prior_dict_DP_D)\n",
    "print()\n",
    "# make a BGMM with similar parameters to the DPMM (i.e., 30 components)\n",
    "arg_prior_dict_D = arg_prior_dict_DP_D.copy()\n",
    "arg_prior_dict_D['weight_concentration_prior_type'] = 'dirichlet_distribution'\n",
    "print('BGMM D:')\n",
    "arg_prior_dict_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the BGMM and fit the distribution\n",
    "DPMM_D = BayesianGaussianMixture(**arg_prior_dict_DP_D,**arg_dict)\n",
    "DPMM_D.fit(this_sample.reshape(-1,1))\n",
    "\n",
    "# define the BGMM and fit the distribution\n",
    "BGMM_D = BayesianGaussianMixture(**arg_prior_dict_D,**arg_dict)\n",
    "BGMM_D.fit(this_sample.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just look at the fits for reference\n",
    "# plt.plot(qx,sf.eval_pdf(qx,DPMM_D),label='DPMM D')\n",
    "# plt.plot(qx,sf.eval_pdf(qx,BGMM_D),label='BGMM D')\n",
    "# plt.hist(this_sample,density=True,edgecolor='k',color='xkcd:sky blue', alpha=0.25)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add these models to the post list\n",
    "this_post_list['post_DPMM_D'] = {'model': DPMM_D,\n",
    "                               'Fmodel': sf.Forward_BGM_Model(DPMM_D)}\n",
    "\n",
    "this_post_list['BGMM_D'] = {'model': BGMM_D,\n",
    "                            'Fmodel': sf.Forward_BGM_Model(BGMM_D)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the post sampled\n",
    "fig_DPMM_BGMM_compare, axes = plt.subplots(1,2)\n",
    "fig_DPMM_BGMM_compare.set_figwidth(10)\n",
    "fig_DPMM_BGMM_compare.set_figheight(5)\n",
    "\n",
    "\n",
    "# plot target \n",
    "axes[0].plot(qx,sf.eval_pdf(qx,obs_dist), ls='--', label='Target',\n",
    "                color='k',alpha=0.4,zorder=-1)\n",
    "\n",
    "# setup colors and stuff\n",
    "these_keys = ['post_DPMM_D','BGMM_D']\n",
    "hatches = {'post_DPMM_D': None, 'BGMM_D': '/'}\n",
    "fill_colors = {'post_DPMM_D': 'C0', 'BGMM_D': 'xkcd:orchid'}\n",
    "edge_colors = {'post_DPMM_D': 'xkcd:sky blue', 'BGMM_D': 'xkcd:dark violet'}\n",
    "alphas = {'post_DPMM_D': 0.85, 'BGMM_D': 0.75}\n",
    "plot_labels = {'post_DPMM_D': 'DPMM', 'BGMM_D': 'BGMM'}\n",
    "\n",
    "# make plot\n",
    "for key in these_keys:\n",
    "    # plot expected posterior for DPMM and BGMM\n",
    "    qy = sf.eval_pdf(qx,this_post_list[key]['model'])\n",
    "    axes[0].plot(qx,qy,label=plot_labels[key], color=fill_colors[key],\n",
    "                 zorder=0,linewidth=3,alpha=0.8)\n",
    "\n",
    "    # bar graph of the weights\n",
    "    # get mean weights\n",
    "    mean_weights = this_post_list[key]['Fmodel'].weight_dist.mean()\n",
    "\n",
    "    # order K components: keep the top 7\n",
    "    topX = 7\n",
    "    large_ind_to_small = np.flip(np.argsort(mean_weights))\n",
    "    print(large_ind_to_small[0:topX])\n",
    "    colorlist = ['xkcd:orangered','xkcd:crimson','xkcd:orchid']+['C0']*(topX-3)\n",
    "    weight_bars = np.arange(topX)+1\n",
    "\n",
    "    # get forward sample to compute weight variances\n",
    "    weight_std = np.sqrt(np.var(this_post_list['post_DPMM']['Fmodel'].weight_dist.rvs(500),\n",
    "                        ddof=1,axis=0))\n",
    "    low_err = np.min(np.array([weight_std,mean_weights]),axis=0)\n",
    "    err_bars = np.array([low_err,weight_std])\n",
    "\n",
    "    # bar plot\n",
    "    axes[1].bar(weight_bars,mean_weights[large_ind_to_small[0:topX]],\n",
    "                color=fill_colors[key],alpha=alphas[key],edgecolor=edge_colors[key],\n",
    "                   linewidth=2,tick_label=None,#large_ind_to_small[0:topX]+1,\n",
    "                    yerr=err_bars[:,large_ind_to_small[0:topX]],\n",
    "                      error_kw={'capsize': 4}, hatch=hatches[key],\n",
    "                    ecolor=edge_colors[key],label=plot_labels[key]\n",
    "                   )\n",
    "axes[1].set_xticklabels([])\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "\n",
    "# set titles:\n",
    "this_alpha = [1]\n",
    "axes[0].set_title('Posterior DPMM vs. BGMM, $\\\\Psi=h_{MISE}^2$')\n",
    "axes[1].set_title('Posterior Average Weights')\n",
    "\n",
    "# labels\n",
    "axes[0].set_xlabel('$q$')\n",
    "axes[1].set_xlabel('Components')\n",
    "# use ylim of first plot\n",
    "\n",
    "# # savefig\n",
    "fig_DPMM_BGMM_compare.tight_layout()\n",
    "this_fig_title = 'fig_DPMM_BGMM_compare.png'\n",
    "fig_all_master[this_fig_title] = fig_DPMM_BGMM_compare\n",
    "\n",
    "# # save just this fig\n",
    "# fig_DPMM_BGMM_compare.savefig('../'+this_fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all the figures are there\n",
    "for key in fig_all_master.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # save all figs\n",
    "# for figfilename in fig_all_master:\n",
    "#     fig_all_master[figfilename].savefig('../'+figfilename,\n",
    "#                                         dpi=250,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Notes (TO BE CLEANED LATER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
